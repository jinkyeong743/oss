import streamlit as st
import numpy as np
from streamlit_drawable_canvas import st_canvas
from scipy.io.wavfile import write

# --------------------------------------------------------------------
# A. í•„ìš” ìƒìˆ˜ì™€ í•¨ìˆ˜ ì •ì˜ 
# --------------------------------------------------------------------

SAMPLE_RATE = 44100 # WAV ì˜¤ë””ì˜¤ ìƒ˜í”Œë§ ë ˆì´íŠ¸
CANVAS_WIDTH = 750 # ìº”ë²„ìŠ¤ ê°€ë¡œ ê¸¸ì´
CANVAS_HEIGHT = 500 # ìº”ë²„ìŠ¤ ì„¸ë¡œ ê¸¸ì´

# X ì¢Œí‘œë¥¼ ìŒ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘í•˜ê¸° ìœ„í•œ ê¸°ë³¸ 7ìŒê³„ (ë„-ë ˆ-ë¯¸-íŒŒ-ì†”-ë¼-ì‹œ)
BASE_NOTES_DIATONIC = ['c', 'd', 'e', 'f', 'g', 'a', 'b'] 

# ì„ ì˜ ë³µì¡ë„ì— ë”°ë¼ ì‚¬ìš©í•  ìŒê³„
SCALE_PENTATONIC = ['c', 'd', 'e', 'g', 'a'] # ì§ì„ ì— ê°€ê¹Œìš´ ì„ 
SCALE_MAJOR = BASE_NOTES_DIATONIC # ì¼ë°˜ì ì¸ ì„ 
SCALE_CHROMATIC = ['c', 'c#', 'd', 'd#', 'e', 'f', 'f#', 'g', 'g#', 'a', 'a#', 'b'] # ë³µì¡í•œ ì„ 

# ë³µì¡ë„ ê¸°ì¤€ê°’
SHARP_HIGH = 0.5  # ë³µì¡í•œ ì„ 
SHARP_MID = 0.25  # ì¤‘ê°„ ìˆ˜ì¤€ì˜ ì„ 

MAX_EXPECTED_LENGTH = 2500 # ì„ ì˜ ìµœëŒ€ ê¸¸ì´

# ê°ì •ë³„ ìŒì•… íŠ¹ì§•(í…œí¬ì™€ ì˜¥íƒ€ë¸Œ)ì„ ì €ì¥í•œ êµ¬ì¡°ì²´
EMOTION_PARAMETERS = {
    "ê¸°ì¨ (Joy)":      {"octave_base": 4, "duration_ratio": 0.5, "description": "ê²½ì¾Œí•˜ê³  ë¹ ë¥¸ í…œí¬ (ê¸°ì¤€ ì˜¥íƒ€ë¸Œ: 4)"},
    "í¬ë§ (Hope)":       {"octave_base": 4, "duration_ratio": 0.8, "description": "ë°ê³  ë³´í†µ ì†ë„ì˜ í…œí¬ (ê¸°ì¤€ ì˜¥íƒ€ë¸Œ: 4)"},
    "í‰ì˜¨ (Serene)":     {"octave_base": 3, "duration_ratio": 1.5, "description": "í¸ì•ˆí•˜ê³  ëŠë¦° í…œí¬ (ê¸°ì¤€ ì˜¥íƒ€ë¸Œ: 3)"},
    "ë¶„ë…¸ (Angry)":      {"octave_base": 3, "duration_ratio": 0.4, "description": "ê°•ë ¬í•˜ê³  ë§¤ìš° ë¹ ë¥¸ í…œí¬ (ê¸°ì¤€ ì˜¥íƒ€ë¸Œ: 3)"},
    "ê²©ë ¬ (Intense)":    {"octave_base": 4, "duration_ratio": 0.5, "description": "ë†’ê³  ë¹ ë¥¸ í…œí¬ (ê¸°ì¤€ ì˜¥íƒ€ë¸Œ: 4)"},
    "ìŠ¬í”” (Sorrow)":     {"octave_base": 2, "duration_ratio": 2.0, "description": "ë§¤ìš° ëŠë¦¬ê³  ë‚®ì€ í…œí¬ (ê¸°ì¤€ ì˜¥íƒ€ë¸Œ: 2)"},
    "ë¶ˆì•ˆ (Anxious)":    {"octave_base": 3, "duration_ratio": 0.7, "description": "ë¶ˆê·œì¹™í•˜ê³  ë¹ ë¥¸ í…œí¬ (ê¸°ì¤€ ì˜¥íƒ€ë¸Œ: 3)"},
}

# íŠ¹ì • ì£¼íŒŒìˆ˜ì™€ íŠ¹ì • ê¸¸ì´ì˜ ì‚¬ì¸íŒŒ ì˜¤ë””ì˜¤ ë°ì´í„° ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def generate_note_wave(note_freq, duration_seconds, amplitude=4096):
    t = np.linspace(0, duration_seconds, int(SAMPLE_RATE * duration_seconds), False)
    audio = amplitude * np.sin(note_freq * 2 * np.pi * t)
    return audio.astype(np.int16)

# ìŒí‘œ ë¬¸ìì—´(c#4)ì„ ì‹¤ì œ ì£¼íŒŒìˆ˜ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def note_to_freq(note_str):
    notes = {
        'c': 261.63, 'c#': 277.18, 'd': 293.66, 'd#': 311.13, 
        'e': 329.63, 'f': 349.23, 'f#': 369.99, 'g': 392.00, 
        'g#': 415.30, 'a': 440.00, 'a#': 466.16, 'b': 493.88
    } # 4ì˜¥íƒ€ë¸Œ ê¸°ì¤€ ì£¼íŒŒìˆ˜
    
    base_note = note_str[:-1].lower()
    try:
        octave = int(note_str[-1])
    except ValueError:
        octave = 4 # 4ì˜¥íƒ€ë¸Œë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì„¤ì •

    freq = notes.get(base_note, 0)
    if freq == 0: return 0
    
    return freq * (2 ** (octave - 4))

# í•˜ë‚˜ì˜ ì„ ì„ ë¶„ì„í•˜ì—¬ í•˜ë‚˜ì˜ ë©œë¡œë””ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def generate_voice_melody(path_coords, params):
    
    coords = np.array(path_coords)
    x = coords[:, 0]
    y = coords[:, 1]
    
    # ê°ì • êµ¬ì¡°ì²´ì˜ ìš”ì†Œ ë¶ˆëŸ¬ì˜¤ê¸°
    octave_base = params["octave_base"]
    note_duration_ratio = params["duration_ratio"]
    
    # ì„  ê¸¸ì´ ê³„ì‚°
    dx = np.diff(x)
    dy = np.diff(y)
    total_length = np.sum(np.sqrt(dx**2 + dy**2)) 
    
    # ê¸°ìš¸ê¸° ë³€í™”ëŸ‰(ë³µì¡ë„) ê³„ì‚°
    slopes = np.divide(dy, dx, out=np.zeros_like(dy, dtype=float), where=dx!=0)
    slope_changes = np.abs(np.diff(slopes))
    sharpness_score = np.mean(slope_changes)
    
    # ì„  ê¸¸ì´ 50í”½ì…€ ë‹¹ ìŒí‘œ í•˜ë‚˜ ìƒì„±
    num_melody_notes = min(32, int(total_length / 50)) 
    if num_melody_notes == 0:
        return None, total_length, 0.0, None

    # ì„  ê¸¸ì´ì— ë”°ë¥¸ ìŒí‘œ ê¸¸ì´ ê²°ì •
    length_ratio = min(total_length / MAX_EXPECTED_LENGTH, 1.5)
    base_duration = 0.3 # ê¸°ë³¸ ìŒí‘œ ê¸¸ì´
    duration_sec = base_duration * length_ratio * note_duration_ratio 
    
    # ë³µì¡ë„ì— ë”°ë¥¸ ìŒê³„ ê²°ì •
    if sharpness_score >= SHARP_HIGH:
        current_scale = SCALE_CHROMATIC
    elif sharpness_score >= SHARP_MID:
        current_scale = SCALE_MAJOR
    else:
        current_scale = SCALE_PENTATONIC
        
    # ì‚¬ìš©í•  ìŒì˜ ê°œìˆ˜
    NUM_SCALE_NOTES = len(current_scale)
    
    # ì´ ì¢Œí‘œ ì¤‘ì—ì„œ ìƒ˜í”Œë§í•  ì¢Œí‘œ ì„ íƒ
    indices_to_sample = np.linspace(0, len(x) - 1, num_melody_notes, dtype=int)
    
    voice_audio = np.array([], dtype=np.int16)
    
    for i in indices_to_sample:
        # xì¢Œí‘œì— ë”°ë¥¸ ìŒ ì´ë¦„ ì„ íƒ
        X_norm = x[i] / CANVAS_WIDTH
        note_index = int(np.clip(X_norm * NUM_SCALE_NOTES, 0, NUM_SCALE_NOTES - 1))
        base_note_name = current_scale[note_index]
        
        # yì¢Œí‘œì— ë”°ë¥¸ ì˜¥íƒ€ë¸Œ ê²°ì •
        Y_inverted_norm = 1.0 - (y[i] / CANVAS_HEIGHT)
        octave_shift = int(np.clip(Y_inverted_norm * 4, 0, 3))
        current_octave = octave_base + octave_shift # 2ì˜¥íƒ€ë¸Œ-7ì˜¥íƒ€ë¸Œ ì‚¬ìš©

        # ìµœì¢… ìŒí‘œ ìƒì„±
        note_str = f'{base_note_name}{current_octave}'
        freq = note_to_freq(note_str)
        
        # ì›¨ì´ë¸Œ ìƒì„±
        note_wave = generate_note_wave(freq, duration_sec)
        voice_audio = np.concatenate((voice_audio, note_wave))

    return voice_audio, total_length, sharpness_score, current_scale

# ì—¬ëŸ¬ ê°œì˜ ì„ ìœ¼ë¡œ ì—¬ëŸ¬ ë©œë¡œë””ë¥¼ ìƒì„±í•˜ê³  ë¯¹ì‹±í•˜ëŠ” í•¨ìˆ˜
def analyze_and_compose_polyphony(all_paths, selected_emotion):
    
    # ê°ì • ë¶ˆëŸ¬ì˜¤ê¸°
    params = EMOTION_PARAMETERS.get(selected_emotion)
    if not params:
        return None, "ì„ íƒëœ ê°ì •ì— ëŒ€í•œ ìŒì•… íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    voice_audios = []
    voice_results = [] # ë¶„ì„ ê²°ê³¼ ì €ì¥ìš©
    
    # ì„  ê°œìˆ˜ë§Œí¼ ë©œë¡œë”” ìƒì„±
    for path_coords in all_paths:
        voice_audio, total_length, sharpness_score, current_scale = generate_voice_melody(path_coords, params)
        
        if voice_audio is not None:
            voice_audios.append(voice_audio)
            
            voice_results.append({
                "length": total_length,
                "sharpness": sharpness_score,
                "scale": current_scale
            })

    if not voice_audios:
        return None, "ìº”ë²„ìŠ¤ì—ì„œ ìœ íš¨í•œ ì„ ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„ ì„ í•˜ë‚˜ ì´ìƒ ê¸¸ê²Œ ê·¸ë ¤ì£¼ì„¸ìš”."
    
    # ë¯¹ì‹±: ë©œë¡œë”” ê¸¸ì´ë¥¼ ë§ì¶”ê³  í•©ì¹œ í›„, ë³¼ë¥¨ì„ ì¡°ì ˆ
    max_len = max(len(audio) for audio in voice_audios)
    mixed_audio_float = np.zeros(max_len, dtype=np.float32)
    
    for audio in voice_audios:
        padded_audio = np.pad(audio, (0, max_len - len(audio)), 'constant')
        mixed_audio_float += padded_audio
        
    # ë³¼ë¥¨ ì¡°ì ˆ
    mixed_audio_float /= len(voice_audios) 
    mixed_audio_int16 = np.clip(mixed_audio_float, -32768, 32767).astype(np.int16)
    
    # WAV íŒŒì¼ ìƒì„±
    output_filename = "output_melody.wav"
    try:
        write(output_filename, SAMPLE_RATE, mixed_audio_int16)
        
        # ë¶„ì„ ê²°ê³¼ í…ìŠ¤íŠ¸ ìƒì„±
        result_text = f"**ì„ íƒ ê°ì •:** {selected_emotion} | ì´ {len(voice_audios)}ê°œ ì„ ì„ ì‚¬ìš©í•´ ë©œë¡œë”” ìƒì„± | "
        result_text += params["description"] + "\n\n"
        result_text += "\n**ê° ì„  ë¶„ì„:**\n"
        for idx, res in enumerate(voice_results):
            scale_name = "íœíƒ€í† ë‹‰ (5ìŒ)" if len(res['scale']) == 5 else ("ì˜¨ìŒê³„ (7ìŒ)" if len(res['scale']) == 7 else "ë°˜ìŒê³„ (12ìŒ)")
            result_text += (
                f"- ì„  {idx+1}: ê¸¸ì´ {res['length']:.0f}px | ë³µì¡ë„ {res['sharpness']:.2f} "
                f"-> {scale_name} ì‚¬ìš©\n"
            )
        
        return output_filename, result_text
    
    except Exception as e:
        return None, f"ìŒì•… íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (ì˜¤ë¥˜: {e})"
    
# --------------------------------------------------------------------
# B. Streamlit UI êµ¬ì„±
# --------------------------------------------------------------------

st.set_page_config(layout="wide", page_title="Drawing to Song ğŸ¶")

st.markdown("""
<style>
.main {
    background-color: #f7f7f7;
}

.stExpander > div:first-child {
    border-radius: 8px;
    background-color: #f0f2f6;
}
.stButton>button {
    background-color: #red !important;
    color: white;
    border-radius: 8px;
    height: 3rem;
    font-size: 1.2rem;
    font-weight: bold;
    border: 2px solid #FFFFFF;
}
.stButton>button:hover {
    color: white;
    border: 2px solid #FFFFFF;
    filter: none;
}
</style>
""", unsafe_allow_html=True)

st.title("ğŸ¼ Melody Canvas")
st.markdown("#### ê·¸ë¦¼ì„ ê·¸ë¦¬ë©´, ë‚˜ë§Œì˜ ìŒì•…ì´ ë©ë‹ˆë‹¤.")

st.subheader("ğŸ¹ ë©œë¡œë”” ìƒì„± ê·œì¹™ í™•ì¸")
with st.expander("ê·œì¹™ ìì„¸íˆ ë³´ê¸°", expanded=False):
    st.markdown("""
    ì´ ìº”ë²„ìŠ¤ëŠ” ê°€ë¡œì¶•(X)ê³¼ ì„¸ë¡œì¶•(Y)ì˜ ìœ„ì¹˜, ê·¸ë¦¬ê³  ì„ ì˜ íŠ¹ì„±ì„ ì•…ë³´ì²˜ëŸ¼ í•´ì„í•©ë‹ˆë‹¤.

    * **ì„ ì˜ ê°€ë¡œì¶• ìœ„ì¹˜**: ì™¼ìª½(ë„)ì—ì„œ ì˜¤ë¥¸ìª½(ì‹œ)ìœ¼ë¡œ ê°ˆìˆ˜ë¡ ê³„ì´ë¦„ì´ ë†’ì•„ì§‘ë‹ˆë‹¤.
    * **ì„ ì˜ ì„¸ë¡œì¶• ìœ„ì¹˜**: ìœ„ìª½(ê³ ìŒ)ì—ì„œ ì•„ë˜ìª½(ì €ìŒ)ìœ¼ë¡œ ê°ˆìˆ˜ë¡ ì˜¥íƒ€ë¸Œê°€ ë‚®ì•„ì§‘ë‹ˆë‹¤.
    * **ì„  ê¸¸ì´**: ì„ ì´ ê¸¸ìˆ˜ë¡ ìŒí‘œê°€ ê¸¸ì–´ì§‘ë‹ˆë‹¤.
    * **ì„ ì˜ ë³µì¡ë„**: ë¶€ë“œëŸ¬ìš´ ì„ ì€ ì•ˆì •ì ì¸ 5ìŒê³„, ë³µì¡í•œ ì„ ì€ ê¸´ì¥ê° ìˆëŠ” 12ìŒê³„ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    * **ê°ì •**: ì„ íƒí•œ ê°ì •ì— ë”°ë¼ì„œ ê³¡ì˜ ë¶„ìœ„ê¸°ê°€ ë°”ë€ë‹ˆë‹¤.
    """)

st.markdown("---")
st.subheader("2. ê°ì •ê³¼ ìƒ‰ê¹” ì„¤ì •í•˜ê¸°")

col1, col2, col_space = st.columns([2, 2, 4])
with col1:
    selected_emotion = st.selectbox("ê°ì • ì„ íƒ", EMOTION_PARAMETERS.keys())
with col2:
    stroke_color = st.color_picker("ì„  ìƒ‰ê¹” ì„ íƒ", "#FF4B4B") # ê¸°ë³¸ ìƒ‰ìƒì„ ëˆˆì— ë„ê²Œ ë³€ê²½

# --- ìº”ë²„ìŠ¤ êµ¬ì—­ ---
st.subheader("3. ìº”ë²„ìŠ¤ì— ììœ ë¡­ê²Œ ê·¸ë¦¼ ê·¸ë¦¬ê¸°")

st.markdown(
    "<div style='text-align: center; color: #6C5CE7; padding-bottom: 5px;'>â¬†ï¸ ë†’ì€ ì˜¥íƒ€ë¸Œ</div>",
    unsafe_allow_html=True
)

col_left_marker, col_canvas, col_right_marker = st.columns([1, 8, 1])

with col_canvas:
    canvas_result = st_canvas(
        fill_color="rgba(255, 165, 0, 0.0)",
        stroke_width=5,
        stroke_color=stroke_color,
        background_color="#FFFFFF",
        height=CANVAS_HEIGHT,
        width=CANVAS_WIDTH,
        drawing_mode="freedraw",
        update_streamlit=True,
        key="canvas",
    )

col_footer_left, col_footer_center, col_footer_right = st.columns([1, 8, 1])
with col_footer_center:
    st.markdown(
        "<div style='display: flex; justify-content: space-between; width: 100%; padding-top: 10px;'>"
        "<span style='color: #555; font-weight: bold;'>â¬…ï¸ ë„, C</span>"
        "<span style='color: #555; font-weight: bold;'> ì‹œ, B â¡ï¸</span>"
        "</div>", 
        unsafe_allow_html=True
    )

st.markdown(
    "<div style='text-align: center; color: #6C5CE7; padding-top: 5px;'>â¬‡ï¸ ë‚®ì€ ì˜¥íƒ€ë¸Œ</div>",
    unsafe_allow_html=True
)

st.markdown("---")

# ë©œë¡œë”” ìƒì„± ë²„íŠ¼
if st.button("ğŸ¶ 4. ë©œë¡œë”” ìƒì„± ë° ì¬ìƒ", use_container_width=True, type="primary"):
    if canvas_result.json_data: 
        
        all_objects = canvas_result.json_data.get('objects', [])
        all_paths_coords = []
        
        # 1. ëª¨ë“  ì„  ê°ì²´ì˜ ì¢Œí‘œë¥¼ ë¶„ë¦¬í•˜ì—¬ ë©œë¡œë”” ìƒì„±ìš© ë°ì´í„°ë¡œ ë³€í™˜
        if all_objects:
            for obj in all_objects:
                if obj.get('type') == 'path':
                    path_array = obj.get('path', [])
                    drawing_points = []
                    
                    for command in path_array:
                        command_type = command[0]
                        
                        # ì¢Œí‘œ ì¶”ì¶œ (M, L, C, Q ëª…ë ¹ì–´ì—ì„œ ëª¨ë“  ì  ì¶”ì¶œ)
                        if command_type == 'M' or command_type == 'L':
                            if len(command) >= 3:
                                drawing_points.append((command[1], command[2]))
                        elif command_type == 'C':
                            if len(command) >= 7:
                                drawing_points.append((command[1], command[2])) 
                                drawing_points.append((command[3], command[4])) 
                                drawing_points.append((command[5], command[6])) 
                        elif command_type == 'Q':
                            if len(command) >= 5:
                                drawing_points.append((command[1], command[2])) 
                                drawing_points.append((command[3], command[4])) 
                    
                    if drawing_points:
                        all_paths_coords.append(drawing_points)
        
        # 2. ì¶”ì¶œëœ ëª¨ë“  ì„  ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶„ì„ í•¨ìˆ˜ì— ì „ë‹¬
        if all_paths_coords:
            
            # ê²°ê³¼ ìƒì„±
            st.subheader("5. ìƒì„± ê²°ê³¼")

            with st.spinner(f"ì´ {len(all_paths_coords)}ê°œ ì„ ì˜ ë©œë¡œë””ë¥¼ ë¯¹ì‹± ì¤‘ì…ë‹ˆë‹¤... ğŸµ"):
                audio_file_path, analysis_result = analyze_and_compose_polyphony(all_paths_coords, selected_emotion)
            
            if audio_file_path:
                st.success("âœ… ë©œë¡œë”” ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.markdown("---")
                st.markdown(analysis_result)
                st.markdown("---")
                try:
                    audio_bytes = open(audio_file_path, 'rb').read()
                    st.audio(audio_bytes, format='audio/wav')
                except FileNotFoundError:
                    st.error("ì˜¤ë””ì˜¤ íŒŒì¼ ì¬ìƒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            elif analysis_result:
                st.error(f"âŒ ìƒì„± ì‹¤íŒ¨: {analysis_result}")
            
        else:
            st.warning("âš ï¸ ìº”ë²„ìŠ¤ì— ì„ ì´ ì¸ì‹ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„ ì„ í•˜ë‚˜ ì´ìƒ ê¸¸ê²Œ ê·¸ë ¤ì£¼ì„¸ìš”!")

    else:
        st.warning("âš ï¸ ìº”ë²„ìŠ¤ ë°ì´í„°ë¥¼ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.") 